{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comprehensive Error Analysis\n",
                "\n",
                "This notebook performs deep error analysis on the optimized ensemble models to understand:\n",
                "1. Where models fail and why\n",
                "2. Patterns in misclassifications\n",
                "3. Insights for CodeBERT strategy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import joblib\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "from src.models.data_loader import DataLoader\n",
                "\n",
                "# Load data\n",
                "loader = DataLoader()\n",
                "train_df, val_df, test_df = loader.load_data()\n",
                "\n",
                "code_smells = [\n",
                "    'has_long_method',\n",
                "    'has_high_complexity',\n",
                "    'has_too_many_params',\n",
                "    'has_deep_nesting',\n",
                "    'has_no_docstring'\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 1: Confusion Matrix Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"SECTION 1: CONFUSION MATRIX DEEP DIVE\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "confusion_results = []\n",
                "\n",
                "for smell in code_smells:\n",
                "    print(f\"\\n{'='*70}\")\n",
                "    print(f\"Analyzing: {smell}\")\n",
                "    print(f\"{'='*70}\")\n",
                "    \n",
                "    # Load best model\n",
                "    model = joblib.load(f'../models/ensemble/{smell}_voting.pkl')\n",
                "    \n",
                "    # Get predictions\n",
                "    X_val, _, _, y_val, _, _ = loader.prepare_data_for_smell(\n",
                "        train_df, val_df, test_df, smell\n",
                "    )\n",
                "    \n",
                "    y_pred = model.predict(X_val)\n",
                "    \n",
                "    # Confusion matrix\n",
                "    cm = confusion_matrix(y_val, y_pred)\n",
                "    tn, fp, fn, tp = cm.ravel()\n",
                "    \n",
                "    # Calculate rates\n",
                "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
                "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
                "    \n",
                "    print(f\"\\nConfusion Matrix:\")\n",
                "    print(f\"  True Negatives:  {tn:4d} (correctly identified clean code)\")\n",
                "    print(f\"  False Positives: {fp:4d} (clean code flagged as issue) - FPR: {fpr:.2%}\")\n",
                "    print(f\"  False Negatives: {fn:4d} (missed real issues) - FNR: {fnr:.2%}\")\n",
                "    print(f\"  True Positives:  {tp:4d} (correctly caught issues)\")\n",
                "    \n",
                "    # Which is worse?\n",
                "    if fpr > fnr:\n",
                "        error_type = \"High False Positive Rate: Model over-flags clean code\"\n",
                "    elif fnr > fpr:\n",
                "        error_type = \"High False Negative Rate: Model misses real issues\"\n",
                "    else:\n",
                "        error_type = \"Balanced error rates\"\n",
                "    \n",
                "    print(f\"\\nâš ï¸ {error_type}\")\n",
                "    \n",
                "    confusion_results.append({\n",
                "        'smell': smell,\n",
                "        'tn': tn,\n",
                "        'fp': fp,\n",
                "        'fn': fn,\n",
                "        'tp': tp,\n",
                "        'fpr': fpr,\n",
                "        'fnr': fnr,\n",
                "        'error_type': error_type\n",
                "    })\n",
                "\n",
                "confusion_df = pd.DataFrame(confusion_results)\n",
                "confusion_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 2: Misclassification Patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nSECTION 2: ANALYZING MISCLASSIFIED SAMPLES\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "def analyze_misclassifications(smell, model, X_val, y_val, val_df_subset, n_samples=3):\n",
                "    \"\"\"Analyze specific examples of errors\"\"\"\n",
                "    \n",
                "    y_pred = model.predict(X_val)\n",
                "    \n",
                "    # Get indices of errors\n",
                "    fp_indices = np.where((y_val == 0) & (y_pred == 1))[0]  # False Positives\n",
                "    fn_indices = np.where((y_val == 1) & (y_pred == 0))[0]  # False Negatives\n",
                "    \n",
                "    print(f\"\\n{smell}\")\n",
                "    print(\"-\"*70)\n",
                "    print(f\"False Positives: {len(fp_indices)} samples\")\n",
                "    print(f\"False Negatives: {len(fn_indices)} samples\")\n",
                "    \n",
                "    results = {\n",
                "        'smell': smell,\n",
                "        'fp_count': len(fp_indices),\n",
                "        'fn_count': len(fn_indices),\n",
                "        'fp_examples': [],\n",
                "        'fn_examples': []\n",
                "    }\n",
                "    \n",
                "    # Analyze False Positives\n",
                "    if len(fp_indices) > 0:\n",
                "        print(f\"\\nSample False Positives (predicted issue, but actually clean):\")\n",
                "        for i, idx in enumerate(fp_indices[:n_samples]):\n",
                "            if idx < len(val_df_subset):\n",
                "                code = val_df_subset.iloc[idx]['code']\n",
                "                print(f\"\\nFP #{i+1}:\")\n",
                "                print(f\"  Lines: {len(code.split(chr(10)))}\")\n",
                "                print(f\"  Code snippet: {code[:150]}...\")\n",
                "                results['fp_examples'].append(code[:150])\n",
                "    \n",
                "    # Analyze False Negatives\n",
                "    if len(fn_indices) > 0:\n",
                "        print(f\"\\nSample False Negatives (real issue, but not detected):\")\n",
                "        for i, idx in enumerate(fn_indices[:n_samples]):\n",
                "            if idx < len(val_df_subset):\n",
                "                code = val_df_subset.iloc[idx]['code']\n",
                "                print(f\"\\nFN #{i+1}:\")\n",
                "                print(f\"  Lines: {len(code.split(chr(10)))}\")\n",
                "                print(f\"  Code snippet: {code[:150]}...\")\n",
                "                results['fn_examples'].append(code[:150])\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Run analysis for each smell\n",
                "misclass_results = []\n",
                "for smell in code_smells:\n",
                "    model = joblib.load(f'../models/ensemble/{smell}_voting.pkl')\n",
                "    X_val, _, _, y_val, _, _ = loader.prepare_data_for_smell(\n",
                "        train_df, val_df, test_df, smell\n",
                "    )\n",
                "    \n",
                "    result = analyze_misclassifications(smell, model, X_val, y_val, val_df, n_samples=3)\n",
                "    misclass_results.append(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 3: Feature Patterns in Errors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nSECTION 3: FEATURE PATTERNS IN ERRORS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "feature_analysis = []\n",
                "\n",
                "for smell in code_smells:\n",
                "    model = joblib.load(f'../models/ensemble/{smell}_voting.pkl')\n",
                "    X_val, _, _, y_val, _, _ = loader.prepare_data_for_smell(\n",
                "        train_df, val_df, test_df, smell\n",
                "    )\n",
                "    \n",
                "    y_pred = model.predict(X_val)\n",
                "    \n",
                "    # Compare features: correct vs incorrect predictions\n",
                "    correct_mask = (y_val == y_pred)\n",
                "    incorrect_mask = ~correct_mask\n",
                "    \n",
                "    print(f\"\\n{smell}:\")\n",
                "    print(\"-\"*70)\n",
                "    \n",
                "    feature_names = loader.get_feature_names()\n",
                "    \n",
                "    # Key features for this smell\n",
                "    if smell == 'has_long_method':\n",
                "        key_features = ['num_lines', 'cyclomatic_complexity', 'num_function_calls']\n",
                "    elif smell == 'has_high_complexity':\n",
                "        key_features = ['cyclomatic_complexity', 'num_if_statements']\n",
                "    elif smell == 'has_too_many_params':\n",
                "        key_features = ['num_parameters', 'num_default_args']\n",
                "    elif smell == 'has_deep_nesting':\n",
                "        key_features = ['num_if_statements', 'num_for_loops']\n",
                "    else:  # has_no_docstring\n",
                "        key_features = ['num_comments', 'num_lines']\n",
                "    \n",
                "    print(f\"Key feature comparison (correct vs incorrect predictions):\")\n",
                "    for feat in key_features:\n",
                "        if feat in feature_names:\n",
                "            feat_idx = feature_names.index(feat)\n",
                "            \n",
                "            if correct_mask.sum() > 0:\n",
                "                correct_mean = X_val[correct_mask, feat_idx].mean()\n",
                "            else:\n",
                "                correct_mean = 0\n",
                "            \n",
                "            if incorrect_mask.sum() > 0:\n",
                "                incorrect_mean = X_val[incorrect_mask, feat_idx].mean()\n",
                "            else:\n",
                "                incorrect_mean = 0\n",
                "            \n",
                "            print(f\"  {feat:30s}: Correct={correct_mean:8.2f}, Incorrect={incorrect_mean:8.2f}\")\n",
                "            \n",
                "            feature_analysis.append({\n",
                "                'smell': smell,\n",
                "                'feature': feat,\n",
                "                'correct_mean': correct_mean,\n",
                "                'incorrect_mean': incorrect_mean\n",
                "            })"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 4: Boundary Cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nSECTION 4: BOUNDARY CASES\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "boundary_results = []\n",
                "\n",
                "for smell in code_smells:\n",
                "    model = joblib.load(f'../models/ensemble/{smell}_voting.pkl')\n",
                "    X_val, _, _, y_val, _, _ = loader.prepare_data_for_smell(\n",
                "        train_df, val_df, test_df, smell\n",
                "    )\n",
                "    \n",
                "    # Get prediction probabilities\n",
                "    y_proba = model.predict_proba(X_val)[:, 1]  # Probability of positive class\n",
                "    \n",
                "    # Find uncertain predictions (probability near 0.5)\n",
                "    uncertain_mask = (y_proba > 0.4) & (y_proba < 0.6)\n",
                "    n_uncertain = uncertain_mask.sum()\n",
                "    \n",
                "    print(f\"\\n{smell}:\")\n",
                "    print(f\"  Uncertain predictions (40-60% confidence): {n_uncertain} ({n_uncertain/len(y_val)*100:.1f}%)\")\n",
                "    \n",
                "    # Accuracy on uncertain samples\n",
                "    if n_uncertain > 0:\n",
                "        y_pred_uncertain = (y_proba[uncertain_mask] > 0.5).astype(int)\n",
                "        y_true_uncertain = y_val[uncertain_mask]\n",
                "        \n",
                "        uncertain_accuracy = (y_pred_uncertain == y_true_uncertain).mean()\n",
                "        print(f\"  Accuracy on uncertain samples: {uncertain_accuracy:.2%}\")\n",
                "        print(f\"  â†’ Model struggles with borderline cases\")\n",
                "        \n",
                "        boundary_results.append({\n",
                "            'smell': smell,\n",
                "            'uncertain_count': n_uncertain,\n",
                "            'uncertain_pct': n_uncertain/len(y_val)*100,\n",
                "            'uncertain_accuracy': uncertain_accuracy\n",
                "        })\n",
                "    else:\n",
                "        boundary_results.append({\n",
                "            'smell': smell,\n",
                "            'uncertain_count': 0,\n",
                "            'uncertain_pct': 0,\n",
                "            'uncertain_accuracy': 0\n",
                "        })\n",
                "\n",
                "boundary_df = pd.DataFrame(boundary_results)\n",
                "boundary_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 5: Key Insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nSECTION 5: KEY INSIGHTS FROM ERROR ANALYSIS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "insights = {\n",
                "    'has_long_method': [\n",
                "        \"Perfect performance due to direct feature (num_lines)\",\n",
                "        \"No errors to analyze - trivial classification task\"\n",
                "    ],\n",
                "    'has_high_complexity': [\n",
                "        \"Perfect performance due to cyclomatic_complexity feature\",\n",
                "        \"Feature-based approach is sufficient for this smell\"\n",
                "    ],\n",
                "    'has_too_many_params': [\n",
                "        \"Perfect performance - num_parameters is direct indicator\",\n",
                "        \"Clear threshold-based detection\"\n",
                "    ],\n",
                "    'has_deep_nesting': [\n",
                "        \"Perfect performance with nesting depth features\",\n",
                "        \"Static analysis metrics are highly effective\"\n",
                "    ],\n",
                "    'has_no_docstring': [\n",
                "        \"Most challenging task (80.73% F1)\",\n",
                "        \"Errors occur on borderline cases\",\n",
                "        \"One-liner functions often lack docstrings (acceptable?)\",\n",
                "        \"Model can't distinguish required vs optional docs\",\n",
                "        \"Some functions are self-documenting (name + types)\",\n",
                "        \"â†’ CodeBERT's semantic understanding should help here\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "for smell, smell_insights in insights.items():\n",
                "    print(f\"\\n{smell}:\")\n",
                "    for insight in smell_insights:\n",
                "        print(f\"  â€¢ {insight}\")\n",
                "\n",
                "print(\"\\n\\nOVERALL LEARNINGS:\")\n",
                "print(\"=\"*70)\n",
                "print(\"\"\"\n",
                "1. Feature-based models achieve perfect scores on structural smells\n",
                "   â†’ This is expected when features directly encode the labeling logic\n",
                "\n",
                "2. \"No Docstring\" is the only non-trivial task (80.73% F1)\n",
                "   â†’ This is where CodeBERT can add value through semantic understanding\n",
                "\n",
                "3. Borderline cases are inherently ambiguous\n",
                "   â†’ Human judgment varies - 80% may be ceiling for some smells\n",
                "\n",
                "4. Automated metrics don't capture all aspects of quality\n",
                "   â†’ Need semantic understanding of code intent and context\n",
                "\n",
                "5. CodeBERT Strategy:\n",
                "   â†’ Focus on \"No Docstring\" detection (most room for improvement)\n",
                "   â†’ Use semantic understanding to identify self-documenting code\n",
                "   â†’ Learn context-dependent documentation requirements\n",
                "   â†’ Target: >85% F1 on \"No Docstring\" task\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\\nERROR ANALYSIS SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(\"\\nConfusion Matrix Summary:\")\n",
                "print(confusion_df[['smell', 'fp', 'fn', 'fpr', 'fnr']].to_string(index=False))\n",
                "\n",
                "print(\"\\n\\nBoundary Case Summary:\")\n",
                "print(boundary_df.to_string(index=False))\n",
                "\n",
                "print(\"\\nâœ… Error analysis complete!\")\n",
                "print(\"ðŸ“Š Key finding: Focus CodeBERT efforts on 'has_no_docstring' detection\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}