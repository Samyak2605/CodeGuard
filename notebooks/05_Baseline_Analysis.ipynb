{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Baseline Model Analysis\n",
                "\n",
                "This notebook analyzes the performance of the XGBoost baseline models trained on Day 5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Load results\n",
                "results = pd.read_csv('../results/baseline_results.csv')\n",
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Performance Comparison Bar Chart\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# Accuracy\n",
                "axes[0, 0].barh(results['smell'], results['val_accuracy'], color='steelblue')\n",
                "axes[0, 0].set_xlabel('Validation Accuracy')\n",
                "axes[0, 0].set_title('Baseline Accuracy by Code Smell')\n",
                "axes[0, 0].set_xlim(0, 1.1)\n",
                "\n",
                "# F1 Score\n",
                "axes[0, 1].barh(results['smell'], results['val_f1'], color='coral')\n",
                "axes[0, 1].set_xlabel('Validation F1 Score')\n",
                "axes[0, 1].set_title('Baseline F1 Score by Code Smell')\n",
                "axes[0, 1].set_xlim(0, 1.1)\n",
                "\n",
                "# ROC-AUC\n",
                "axes[1, 0].barh(results['smell'], results['val_roc_auc'], color='mediumseagreen')\n",
                "axes[1, 0].set_xlabel('Validation ROC-AUC')\n",
                "axes[1, 0].set_title('Baseline ROC-AUC by Code Smell')\n",
                "axes[1, 0].set_xlim(0, 1.1)\n",
                "\n",
                "# Training Time\n",
                "axes[1, 1].barh(results['smell'], results['training_time_seconds'], color='orchid')\n",
                "axes[1, 1].set_xlabel('Training Time (seconds)')\n",
                "axes[1, 1].set_title('Training Time by Code Smell')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/baseline_comparison.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Identify best and worst performing smells\n",
                "print(\"PERFORMANCE ANALYSIS:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nBest performing (by F1): {results.loc[results['val_f1'].idxmax(), 'smell']}\")\n",
                "print(f\"F1 Score: {results['val_f1'].max():.4f}\")\n",
                "\n",
                "print(f\"\\nWorst performing (by F1): {results.loc[results['val_f1'].idxmin(), 'smell']}\")\n",
                "print(f\"F1 Score: {results['val_f1'].min():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Analysis insights\n",
                "print(\"\\n\\nKEY INSIGHTS:\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Check if no_docstring is hardest\n",
                "if results[results['smell'] == 'has_no_docstring']['val_f1'].values[0] < 0.90:\n",
                "    print(\"⚠️ 'No Docstring' detection is challenging (F1 < 0.90)\")\n",
                "    print(\"   Reason: We excluded direct docstring features to prevent leakage.\")\n",
                "\n",
                "# Check for perfect scores\n",
                "perfect_smells = results[results['val_f1'] > 0.99]['smell'].tolist()\n",
                "if perfect_smells:\n",
                "    print(f\"✅ Perfect/Near-Perfect detection for: {', '.join(perfect_smells)}\")\n",
                "    print(\"   Reason: The engineered features (e.g. Radon complexity) align perfectly with the labeling logic.\")\n",
                "\n",
                "# Overall baseline quality\n",
                "avg_f1 = results['val_f1'].mean()\n",
                "print(f\"\\nOverall Average F1: {avg_f1:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}